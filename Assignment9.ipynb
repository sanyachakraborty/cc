{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f14db-c0ea-42cf-9af5-2cf7e2f8b0ef",
   "metadata": {
    "trusted": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuation.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribution (excluding stopwords).\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Paragraph\n",
    "original_text = \"\"\"Mental health is just as important as physical healthâ€”it affects how we think, feel, \n",
    "and act. Taking care of our mind helps us handle stress, build strong relationships, and make good decisions. \n",
    "Regular self-care, rest, and open conversations can go a long way in maintaining emotional well-being. \n",
    "It's okay to seek help when things feel overwhelming. Prioritizing mental health is a sign of strength, not weakness.\"\"\"\n",
    "\n",
    "# 1. Lowercase and remove punctuation\n",
    "text_lower = original_text.lower()\n",
    "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# 2. Tokenize\n",
    "words = word_tokenize(text_clean)\n",
    "sentences = sent_tokenize(original_text)\n",
    "\n",
    "# 3. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 4. Word frequency distribution\n",
    "freq_dist = FreqDist(filtered_words)\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "print(\"Word Frequency:\")\n",
    "for word, freq in freq_dist.items():\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "\n",
    "Q2: Stemming and Lemmatization\n",
    "1. Take the tokenized words from Question 1 (after stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
    "4. Compare and display results of both techniques.\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "porter_stems = [ps.stem(word) for word in filtered_words]\n",
    "lancaster_stems = [ls.stem(word) for word in filtered_words]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Porter Stemmer:\", porter_stems)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stems)\n",
    "print(\"Lemmatized:\", lemmas)\n",
    "\n",
    "\n",
    "Q3. Regular Expressions and Text Splitting\n",
    "1. Take the original text from Question 1.\n",
    "2. Use regular expressions to:\n",
    "a. Extract all words with more than 5 letters.\n",
    "b. Extract all numbers (if any exist).\n",
    "c. Extract all capitalized words.\n",
    "3. Use text splitting techniques to:\n",
    "a. Split the text into words containing only alphabets.\n",
    "b. Extract words starting with a vowel.\n",
    "\n",
    "import re\n",
    "\n",
    "# a. Words with more than 5 letters\n",
    "long_words = re.findall(r'\\b[a-zA-Z]{6,}\\b', original_text)\n",
    "\n",
    "# b. Extract numbers\n",
    "numbers = re.findall(r'\\d+', original_text)\n",
    "\n",
    "# c. Extract capitalized words\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', original_text)\n",
    "\n",
    "# d. Words containing only alphabets\n",
    "alphabet_words = re.findall(r'\\b[a-zA-Z]+\\b', original_text)\n",
    "\n",
    "# e. Words starting with a vowel\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU][a-zA-Z]*\\b', original_text)\n",
    "\n",
    "print(\"Words > 5 letters:\", long_words)\n",
    "print(\"Numbers:\", numbers)\n",
    "print(\"Capitalized Words:\", capitalized_words)\n",
    "print(\"Alphabet-only Words:\", alphabet_words)\n",
    "print(\"Words Starting with Vowel:\", vowel_words)\n",
    "\n",
    "\n",
    "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
    "1. Take original text from Question 1.\n",
    "2. Write a custom tokenization function that:\n",
    "a. Removes punctuation and special symbols but keeps contractions.\n",
    "b. Keeps hyphenated words as a single token.\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact.\n",
    "3. Use Regex Substitutions (re.sub) to:\n",
    "a. Replace email addresses with <EMAIL>.\n",
    "b. Replace URLs with <URL>.\n",
    "c. Replace phone numbers with <PHONE>.\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b', '<EMAIL>', text)\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    # Replace phone numbers\n",
    "    text = re.sub(r'(\\+?\\d{1,3}[ -]?)?\\d{3}[- ]?\\d{3}[- ]?\\d{4}', '<PHONE>', text)\n",
    "\n",
    "    # Remove unwanted punctuation (but keep hyphens and contractions)\n",
    "    text = re.sub(r\"[^\\w\\s\\-']\", ' ', text)\n",
    "\n",
    "    # Tokenize: keep decimals, hyphenated words, contractions\n",
    "    tokens = re.findall(r\"\\d+\\.\\d+|\\w+(?:-\\w+)*|'[a-z]+|\\w+\", text)\n",
    "    return tokens\n",
    "\n",
    "custom_tokens = custom_tokenize(original_text)\n",
    "print(\"Custom Tokens:\", custom_tokens)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
