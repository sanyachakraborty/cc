{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ef81a-b8b0-45d1-b5a1-e06df542a56d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Q1. Write a unique paragraph (5–6 sentences) about your favorite topic.\n",
    "Let's say your favorite topic is Technology\n",
    "Paragraph:\n",
    "Technology has transformed the world into a global village. With the rise of artificial intelligence and machine learning, machines are becoming more intelligent every day. Smartphones have become essential tools for both communication and productivity. Cloud computing allows people to access their data from anywhere in the world. Innovations like virtual reality and augmented reality are redefining entertainment and education.\n",
    "\n",
    "Q1 Solution:\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure required NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "paragraph = \"\"\"Technology has transformed the world into a global village. With the rise of artificial intelligence and machine learning, machines are becoming more intelligent every day. Smartphones have become essential tools for both communication and productivity. Cloud computing allows people to access their data from anywhere in the world. Innovations like virtual reality and augmented reality are redefining entertainment and education.\"\"\"\n",
    "\n",
    "# 1. Lowercase and remove punctuation\n",
    "lowered = paragraph.lower()\n",
    "cleaned = re.sub(r'[^\\w\\s]', '', lowered)\n",
    "\n",
    "# 2. Tokenization\n",
    "word_tokens = word_tokenize(cleaned)\n",
    "sent_tokens = sent_tokenize(paragraph)\n",
    "\n",
    "# 3. Split vs word_tokenize\n",
    "split_tokens = cleaned.split()\n",
    "print(\"Split tokens:\", split_tokens[:10])\n",
    "print(\"Word_tokenize tokens:\", word_tokens[:10])\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "# 5. Word frequency (excluding stopwords)\n",
    "freq_dist = Counter(filtered_words)\n",
    "print(\"Word Frequency Distribution (no stopwords):\", freq_dist)\n",
    "\n",
    "\n",
    "Q2. Using the same paragraph from Q1:\n",
    "Q2 Solution:\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 1. Extract alphabetic words only\n",
    "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', cleaned)\n",
    "\n",
    "# 2. Remove stop words\n",
    "filtered_alpha = [w for w in alphabetic_words if w not in stop_words]\n",
    "\n",
    "# 3. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_alpha]\n",
    "\n",
    "# 4. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_alpha]\n",
    "\n",
    "# 5. Comparison\n",
    "print(\"Stemmed Words:\", stemmed[:10])\n",
    "print(\"Lemmatized Words:\", lemmatized[:10])\n",
    "\n",
    "# Explanation:\n",
    "print(\"\\nExplanation: Stemming is faster but less accurate (e.g., 'machines' becomes 'machin'), while Lemmatization gives meaningful roots (e.g., 'machines' → 'machine'). Prefer lemmatization for tasks needing correct grammar or human readability.\")\n",
    "\n",
    "Q3. Choose 3 short texts and analyze with BoW & TF-IDF\n",
    "Q3 Solution:\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "texts = [\n",
    "    \"The phone has excellent battery life and a sleek design.\",\n",
    "    \"The camera quality is poor, and the phone lags frequently.\",\n",
    "    \"Great value for money and smooth user interface.\"\n",
    "]\n",
    "\n",
    "# 1. Bag of Words\n",
    "count_vect = CountVectorizer()\n",
    "bow_matrix = count_vect.fit_transform(texts)\n",
    "print(\"Bag of Words:\\n\", bow_matrix.toarray())\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vect.fit_transform(texts)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# 3. Top 3 keywords\n",
    "import numpy as np\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\nTop keywords for Text {i+1}:\")\n",
    "    row = tfidf_matrix[i].toarray()[0]\n",
    "    top_indices = np.argsort(row)[-3:][::-1]\n",
    "    for idx in top_indices:\n",
    "        print(f\"{feature_names[idx]}: {row[idx]:.3f}\")\n",
    "\n",
    "\n",
    "Q4. Compare two technologies\n",
    "Q4 Solution:\n",
    "text1 = \"Artificial Intelligence allows machines to mimic human intelligence and perform tasks autonomously.\"\n",
    "text2 = \"Blockchain is a decentralized ledger technology that provides secure and transparent transactions.\"\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return word_tokenize(text)\n",
    "\n",
    "tokens1 = preprocess(text1)\n",
    "tokens2 = preprocess(text2)\n",
    "\n",
    "# a. Jaccard Similarity\n",
    "set1, set2 = set(tokens1), set(tokens2)\n",
    "jaccard_sim = len(set1 & set2) / len(set1 | set2)\n",
    "print(\"Jaccard Similarity:\", jaccard_sim)\n",
    "\n",
    "# b. Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vec.fit_transform([text1, text2])\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "print(\"Cosine Similarity:\", cos_sim)\n",
    "\n",
    "# c. Analysis\n",
    "print(\"Analysis: Cosine Similarity is better for long texts with different vocabularies but similar context, while Jaccard is good for short texts or simple word comparisons.\")\n",
    "\n",
    "Q5. Write and analyze a product review\n",
    "Q5 Solution:\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review = \"I loved the customer support and the user-friendly interface. The product was amazing!\"\n",
    "\n",
    "# 1. Sentiment Analysis\n",
    "blob = TextBlob(review)\n",
    "polarity = blob.sentiment.polarity\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "print(\"Polarity:\", polarity, \"Subjectivity:\", subjectivity)\n",
    "\n",
    "# 2. Classification\n",
    "if polarity > 0.1:\n",
    "    sentiment = \"Positive\"\n",
    "elif polarity < -0.1:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "print(\"Sentiment:\", sentiment)\n",
    "\n",
    "# 3. Word Cloud for positive reviews\n",
    "if sentiment == \"Positive\":\n",
    "    wordcloud = WordCloud(width=600, height=400, background_color='white').generate(review)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud for Positive Review')\n",
    "    plt.show()\n",
    "\n",
    "Q6. Generate text using LSTM or Dense Model\n",
    "Q6 Solution:\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample paragraph\n",
    "train_text = \"Deep learning is a subset of machine learning that uses neural networks to learn from data and make predictions.\"\n",
    "\n",
    "# 1. Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([train_text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "\n",
    "# 2. Create input sequences\n",
    "tokens = tokenizer.texts_to_sequences([train_text])[0]\n",
    "for i in range(1, len(tokens)):\n",
    "    seq = tokens[:i+1]\n",
    "    input_sequences.append(seq)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = np.array(y)\n",
    "\n",
    "# 3. Simple model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_len-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# Text generation\n",
    "seed = \"deep\"\n",
    "next_words = 3\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed += \" \" + output_word\n",
    "\n",
    "print(\"Generated Text:\", seed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
